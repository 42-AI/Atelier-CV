{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "print (tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import matplotlib\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.preprocessing.image import img_to_array\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import argparse\n",
    "import random\n",
    "import pickle\n",
    "import cv2\n",
    "import os\n",
    "from PIL import ImageFile\n",
    "from tqdm import tqdm\n",
    "import h5py\n",
    "import keras\n",
    "import keras.backend as K\n",
    "from keras.models import Model\n",
    "from keras import optimizers\n",
    "from sklearn.manifold import TSNE\n",
    "from keras.applications.resnet50 import ResNet50\n",
    "from keras.preprocessing import image\n",
    "from keras.applications.resnet50 import preprocess_input, decode_predictions\n",
    "from keras import backend as K\n",
    "from keras.layers import Convolution2D, Dense, Input, Flatten, Dropout, MaxPooling2D, BatchNormalization, \\\n",
    "    GlobalMaxPool2D, Concatenate, GlobalMaxPooling2D, GlobalAveragePooling2D, Lambda\n",
    "from keras.applications.inception_v3 import InceptionV3\n",
    "import tensorflow as tf\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.applications.vgg16 import preprocess_input\n",
    "from keras.backend.tensorflow_backend import set_session\n",
    "from collections import defaultdict\n",
    "import time\n",
    "from PIL import Image\n",
    "from functools import partial\n",
    "import functools\n",
    "import cv2 as cv\n",
    "%pylab inline\n",
    "import matplotlib.image as mpimg\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPool2D\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.callbacks import ReduceLROnPlateau\n",
    "from keras.utils import multi_gpu_model\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "from DataGenerator import *\n",
    "from metrics import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "\n",
    "def get_available_gpus():\n",
    "    local_device_protos = device_lib.list_local_devices()\n",
    "    return [x.name for x in local_device_protos if x.device_type == 'GPU']\n",
    "\n",
    "get_available_gpus()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgLst = os.listdir('../wish/train/')\n",
    "nbTrain = len(imgLst)\n",
    "print (nbTrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame()\n",
    "df = pd.read_csv('labels.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of tags, labels and booleans \n",
    "tags = list(df['Unnamed: 0'].values)\n",
    "lbls = list(df['labels'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we iterate in tags list and we keep only the index where kp == 1\n",
    "# tags = [tag for ind, tag in enumerate(tags)\n",
    "#         if kp[ind] == 1]\n",
    "# lbls = [lbl for ind, lbl in enumerate(lbls)\n",
    "#         if kp[ind] == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (len(tags), len(lbls))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pprint import pprint\n",
    "\n",
    "with open('../wish/train.json') as f:\n",
    "    data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete elem which is not in tags\n",
    "# def tags_to_pop(tagsLst):\n",
    "#     for ind, tag in enumerate(tagsLst):\n",
    "#         if tag not in tags:\n",
    "#             del tagsLst[ind]\n",
    "#     return (tagsLst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"le dataset est de {} images\".format(nbTrain))\n",
    "img_to_tags = {int(dic['imageId']):list(map(int, dic['labelId']))\n",
    "               for ind, dic in enumerate(data['annotations'])\n",
    "                if ind < nbTrain}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tag_to_imgs = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in tags:\n",
    "    keyLst = []\n",
    "    for key, value in img_to_tags.items():\n",
    "        if i in value:\n",
    "            keyLst.append(key)\n",
    "    tag_to_imgs[i] = keyLst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imgPath list of path per image \n",
    "imgsPath = ['../wish/train/{}.jpg'.format(key)\n",
    "            for key, value in img_to_tags.items()\n",
    "            if len(value) > 0]\n",
    "# tagsImg list[list] of tags  \n",
    "tagsImg = [value for key, value in img_to_tags.items()\n",
    "           if len(value) > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print ('number of images: {}\\nnumber of tags list: {}'\n",
    "       .format(len(imgsPath),len(tagsImg)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# errImg file which contains the list of images where we can get the shape\n",
    "df3 = pd.read_csv('errImg.csv')\n",
    "err = df3['error'].values\n",
    "pos = df3['position'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print ('number of conflicting file {}'.format(len(err)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb = 30\n",
    "tagsImg = [x for ind, x in enumerate(tagsImg)\n",
    "           if imgsPath[ind] not in err]\n",
    "imgsPath = [x for x in imgsPath\n",
    "            if x not in err]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pour voir l'id maximum \n",
    "ma = 0\n",
    "for img in imgsPath:\n",
    "    nb = int(img.replace('../wish/train/', '').replace('.jpg', ''))\n",
    "    if nb > ma:\n",
    "        ma = nb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print ('the maximum image id {}'.format(ma))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "mlb = MultiLabelBinarizer()\n",
    "cat_tag = mlb.fit_transform(tagsImg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0, 3):\n",
    "    print (tagsImg[i], cat_tag[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train, X_test, y_train, y_test) = train_test_split(imgsPath,\n",
    "                                                      cat_tag,\n",
    "                                                      test_size=0.1,\n",
    "                                                      random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EarlyStopping and ReduceLROnPlateau or LearningRateScheduler \n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau, LearningRateScheduler\n",
    "from keras.applications.resnet50 import ResNet50\n",
    "from keras.applications.mobilenet import MobileNet\n",
    "from keras.preprocessing import image\n",
    "from keras.applications.resnet50 import preprocess_input, decode_predictions\n",
    "\n",
    "from keras.applications.inception_v3 import InceptionV3\n",
    "from keras.layers import Input\n",
    "\n",
    "base_model = ResNet50(weights='imagenet', include_top=False, input_shape=[224, 224, 3])\n",
    "\n",
    "x = base_model.output\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "x = Dense(1024, activation='relu')(x)\n",
    "\n",
    "predictions = Dense(len(mlb.classes_), activation='sigmoid')(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "\n",
    "print ('number of classes: {}'.format(len(mlb.classes_)))\n",
    "\n",
    "from keras.metrics import top_k_categorical_accuracy\n",
    "\n",
    "with tf.device(\"/cpu:0\"):\n",
    "    model = Model(inputs=base_model.input, outputs=predictions)\n",
    "print('number of layers : ' + str(len(model.layers)))\n",
    "\n",
    "for layer in model.layers:\n",
    "    layer.trainable = True\n",
    "    if isinstance(layer, keras.layers.BatchNormalization):\n",
    "        layer.momentum = 0.8\n",
    "    \n",
    "for layer in model.layers[:-20]:\n",
    "    layer.trainable = False\n",
    "    \n",
    "model.compile(optimizer=keras.optimizers.adamax(lr=1e-2),\n",
    "              loss = 'binary_crossentropy',\n",
    "              metrics = [fmeasure, recall, precision])\n",
    "\n",
    "index = 0\n",
    "while (True):\n",
    "    if not os.path.exists('logs/' + str(index)):\n",
    "        os.makedirs('logs/' + str(index))\n",
    "        break ;\n",
    "    index += 1\n",
    "    \n",
    "logsName = './logs/wish' + str(index) # replace index by the name you want\n",
    "tensorboard = keras.callbacks.TensorBoard(log_dir=logsName, \n",
    "                                          histogram_freq=0, \n",
    "                                          batch_size=batch_size, \n",
    "                                          write_graph=True, \n",
    "                                          write_grads=True, \n",
    "                                          write_images=True, \n",
    "                                          embeddings_freq=0,\n",
    "                                          embeddings_layer_names=None, \n",
    "                                          embeddings_metadata=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- https://medium.com/@ilango100/batchnorm-fine-tune-your-booster-bef9f9493e22"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "            'dim': (224,224,3),\n",
    "            'batch_size': 32,\n",
    "            'n_classes': len(mlb.classes_),\n",
    "            'n_channels': 3,\n",
    "            'shuffle': True\n",
    "         }\n",
    "training_generator = DataGenerator(X_train, y_train, **params)\n",
    "validation_generator = DataGenerator(X_test, y_test, **params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 4\n",
    "history = model.fit_generator(generator=training_generator,\n",
    "                    validation_data=validation_generator,\n",
    "                    use_multiprocessing=True,\n",
    "                    workers=3,\n",
    "                    epochs=epochs,\n",
    "                    callbacks=[tensorboard])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# historique pour la loss function\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
